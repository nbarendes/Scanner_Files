To install the llama-2-7b-chat.ggmlv3.q8_0.bin model, follow these steps:


1. Download Model: 
Obtain the model from the Hugging Face repository by visiting TheBloke/Llama-2-7B-Chat-GGML [1].

2. Set Up Llama 2: 
Start by downloading the specific model, llama-2-7b-chat.ggmlv3.q8_0.bin, which requires 10GB RAM [3].

3. Install Dependencies: 
If you plan to run the model locally with Python, install the llama-cpp-python package using the command: pip install llama-cpp-python [4].

4. Usage: 
Integrate the model into your project or script for chatbot development or other natural language processing tasks 

5. To launch application: 
streamlit run Scanner.py
